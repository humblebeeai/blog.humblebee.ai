{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the Humblebee AI Blog","text":""},{"location":"release-notes/","title":"\ud83d\udccc Release Notes","text":""},{"location":"release-notes/#v020-251129-2025-11-29","title":"v0.2.0-251129 (2025-11-29)","text":""},{"location":"release-notes/#whats-changed","title":"What's Changed","text":""},{"location":"release-notes/#features","title":"\u2728 Features","text":"<ul> <li>docs: add Tailscale blog post and author details by @BaratovSokhibjon in https://github.com/humblebeeai/blog.humblebee.ai/pull/2</li> <li>two new blog posts from cv and ds departments by @BaratovSokhibjon in https://github.com/humblebeeai/blog.humblebee.ai/pull/8</li> <li>New Blog - Data Importance for AI development by @BaratovSokhibjon in https://github.com/humblebeeai/blog.humblebee.ai/pull/12</li> </ul>"},{"location":"release-notes/#documentation","title":"\ud83d\udcdd Documentation","text":"<ul> <li>add: new blog post and author by @koyiljon-s in https://github.com/humblebeeai/blog.humblebee.ai/pull/11</li> </ul>"},{"location":"release-notes/#other","title":"\ud83d\udcac Other","text":"<ul> <li>Update logo and links by @MuhammadyusufEngineer in https://github.com/humblebeeai/blog.humblebee.ai/pull/3</li> <li>Remove first test post by @MuhammadyusufEngineer in https://github.com/humblebeeai/blog.humblebee.ai/pull/5</li> <li>remove first blog by @BaratovSokhibjon in https://github.com/humblebeeai/blog.humblebee.ai/pull/6</li> <li>docs: camera spec blog by @inokov in https://github.com/humblebeeai/blog.humblebee.ai/pull/4</li> <li>docs: add new benchmarking blog by @koyiljon-s in https://github.com/humblebeeai/blog.humblebee.ai/pull/7</li> <li>fix formatting and content issues in TabPFN post by @koyiljon-s in https://github.com/humblebeeai/blog.humblebee.ai/pull/10</li> </ul>"},{"location":"release-notes/#new-contributors","title":"New Contributors","text":"<ul> <li>@BaratovSokhibjon made their first contribution in https://github.com/humblebeeai/blog.humblebee.ai/pull/2</li> <li>@MuhammadyusufEngineer made their first contribution in https://github.com/humblebeeai/blog.humblebee.ai/pull/3</li> <li>@inokov made their first contribution in https://github.com/humblebeeai/blog.humblebee.ai/pull/4</li> <li>@koyiljon-s made their first contribution in https://github.com/humblebeeai/blog.humblebee.ai/pull/7</li> </ul> <p>Full Changelog: https://github.com/humblebeeai/blog.humblebee.ai/compare/v0.1.1-250824...v0.2.0-251129</p>"},{"location":"release-notes/#v011-250824-2025-08-24","title":"v0.1.1-250824 (2025-08-24)","text":""},{"location":"release-notes/#whats-changed_1","title":"What's Changed","text":""},{"location":"release-notes/#documentation_1","title":"\ud83d\udcdd Documentation","text":"<ul> <li>feat: Update author avatar, add first blog post, and enhance document\u2026 by @bybatkhuu in https://github.com/humblebeeai/docs.humblebee-blog/pull/1</li> </ul>"},{"location":"release-notes/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@bybatkhuu made their first contribution in https://github.com/humblebeeai/docs.humblebee-blog/pull/1</li> </ul> <p>Full Changelog: https://github.com/humblebeeai/docs.humblebee-blog/compare/v0.1.0-250625...v0.1.1-250824</p>"},{"location":"release-notes/#v010-250625-2025-06-25","title":"v0.1.0-250625 (2025-06-25)","text":"<p>Full Changelog: https://github.com/humblebeeai/docs.humblebee-blog/commits/v0.1.0-250625</p>"},{"location":"blog/","title":"\u270f\ufe0f Blog","text":"<p>This is the blog page. It will list all the blog posts.</p>","tags":["blog"]},{"location":"blog/2025/11/08/tailscale---one-tool-less-infrastructure/","title":"Tailscale - One Tool, Less Infrastructure","text":"<p>You have multiple devices like a laptop, a production server, maybe an edge server. Accessing your work devices from outside your network usually requires port forwarding, dynamic DNS services, or a VPN, this usually raises security concerns as setting those up requires your public IP to be exposed.</p> <p>Tailscale makes this simple. Install it on your devices and sign in. They can now reach each other securely from anywhere.</p>","tags":["blog","VPN","Network","Tailscale"]},{"location":"blog/2025/11/08/tailscale---one-tool-less-infrastructure/#what-is-tailscale","title":"What Is Tailscale?","text":"<p>Tailscale creates a secure network between your devices. Your devices connect directly to each other (peer-to-peer), not through a central server. This means faster speeds and no single point of failure.</p> <p>Install it on your laptop and work server. Sign in with your Google account on both. Now you can <code>ssh servername</code> from a coffee shop and it connects like they're on the same local network.</p> <p>No port forwarding. No complex configs. No networking knowledge required.</p> <p>Beyond being a VPN that actually works, Tailscale replaces several tools: dynamic DNS for device names, ngrok for sharing localhost, and firewall rules for access control.</p>","tags":["blog","VPN","Network","Tailscale"]},{"location":"blog/2025/11/08/tailscale---one-tool-less-infrastructure/#access-devices-by-name","title":"Access Devices by Name","text":"<p>Replaces: Custom DNS servers, memorizing IPs, updating hosts files</p> <p>Setting up Pi-hole or bind9 just to give your devices readable names? Updating DNS records every time you add a machine? Memorizing <code>192.168.1.142</code> because you're too lazy to configure it properly?</p> <p>MagicDNS does this automatically:</p> <p><code>ssh servername</code></p> <p>Every device gets a name. No server to maintain, no records to update, no IPs to remember.</p> <p></p> <p>Type <code>http://nas</code> to reach your NAS. Type <code>ssh work-laptop</code> to reach your work machine. The names just work.</p>","tags":["blog","VPN","Network","Tailscale"]},{"location":"blog/2025/11/08/tailscale---one-tool-less-infrastructure/#share-local-services-publicly","title":"Share Local Services Publicly","text":"<p>Replaces: ngrok, localtunnel, port forwarding</p> <p>You know the drill with ngrok: random URLs that change every time you restart, rate limits on the free tier, paying $8/month if you want a stable subdomain.</p> <p>Tailscale Funnel:</p> <p><code>tailscale funnel 3000</code></p> <p>Your local dev server is now at <code>https://my-laptop.tailnet-name.ts.net</code>. Permanent URL. Free. No rate limits.</p> <p>Testing webhooks from Stripe? Demoing a feature to a client? Running a side project on your work server? One command handles it.</p> <p>If you want to share something with just your team (not the whole internet):</p> <p><code>tailscale serve 3000</code></p> <p>Same thing, but only people on your Tailscale network can access it.</p> <p>\ud83d\udcda Funnel examples \u00b7 Command reference</p>","tags":["blog","VPN","Network","Tailscale"]},{"location":"blog/2025/11/08/tailscale---one-tool-less-infrastructure/#control-who-reaches-what","title":"Control Who Reaches What","text":"<p>Replaces: Firewall rules, iptables configs, security groups</p> <p>SSHing into each server to configure iptables. Documenting which ports are open where. Breaking production because you fat-fingered a rule at 2 AM.</p> <p>Tailscale ACLs are just a JSON file:</p> <pre><code>{\n  \"grants\": [\n    {\n      \"src\": [\"contractor@email.com\"],\n      \"dst\": [\"tag:staging\"],\n      \"ip\": [\"5432\"]\n    }\n  ]\n}\n</code></pre> <p>This contractor can access your staging database (port 5432). Nothing else. When they're done, delete the line. No leftover rules hiding on some forgotten server.</p> <p>Team-based access:</p> <pre><code>{\n  \"grants\": [\n    {\n      \"src\": [\"group:sre\"],\n      \"dst\": [\"tag:prod\"],\n      \"ip\": [\"22\"]\n    }\n  ],\n  \"groups\": {\n    \"group:sre\": [\"alice@company.com\", \"bob@company.com\"]\n  }\n}\n</code></pre> <p>Your SRE team can SSH into production. Everyone else can't. Alice leaves the company? Remove her email. Policy updates everywhere instantly.</p> <p>No more SSH key distribution across 50 servers. No more \"wait, which subnet is staging?\" No more firewall rules that you're afraid to touch because nobody remembers why they exist.</p> <p>\ud83d\udcda ACL examples \u00b7 Grant examples</p>","tags":["blog","VPN","Network","Tailscale"]},{"location":"blog/2025/11/08/tailscale---one-tool-less-infrastructure/#run-services-anywhere","title":"Run Services Anywhere","text":"<p>Replaces: Load balancers, DNS updates, hardcoded hostnames</p> <p>You're running a database on <code>db-server-01</code>. That hostname is hardcoded in your configs. Server dies, you spin up <code>db-server-02</code>, and now you're doing find-replace across your codebase at midnight.</p> <p>Or you set up HAProxy. Configure health checks. Set up DNS. Maintain yet another piece of infrastructure.</p> <p>Tailscale Services:</p> <p><code>tailscale serve --service=svc:postgres-main --tcp 5432 localhost:5432</code></p> <p>Your database is now at <code>postgres-main.corp.ts.net</code>. Server dies? Run the same command on a new machine. The DNS name stays the same. Your apps reconnect automatically.</p> <p>Multi-region example:</p> <pre><code># Primary (US)\ntailscale serve --service=svc:db-primary --tcp 5432 localhost:5432\n\n# Replica (EU)\ntailscale serve --service=svc:db-replica --tcp 5432 localhost:5432\n</code></pre> <p>Apps write to <code>db-primary.corp.ts.net</code> and read from <code>db-replica.corp.ts.net</code>. Primary goes down? Promote the replica and re-advertise it as primary. No config changes needed anywhere.</p> <p>You can also control who accesses these services:</p> <pre><code>{\n  \"grants\": [\n    {\n      \"src\": [\"group:backend-team\"],\n      \"dst\": [\"svc:postgres-main\"],\n      \"ip\": [\"5432\"]\n    }\n  ]\n}\n</code></pre> <p>Backend team can reach the database. Frontend team gets blocked automatically.</p> <p>\ud83d\udcda Services docs \u00b7 Configuration guide</p>","tags":["blog","VPN","Network","Tailscale"]},{"location":"blog/2025/11/08/tailscale---one-tool-less-infrastructure/#no-more-traditional-vpns","title":"No More Traditional VPNs","text":"<p>Central VPN server that's a bottleneck and single point of failure. Slow speeds because everything routes through one location. Complex setup with certificates and subnet planning. \"VPN is down\" means everyone's locked out.</p> <p>Tailscale creates direct peer-to-peer connections. Your laptop talks directly to your server. No central bottleneck. Uses WireGuard encryption without the WireGuard complexity.</p> <p>Setup is install, sign in, done. Takes 2 minutes. Devices can still connect to each other even if Tailscale's coordination server has issues (it handles the initial introduction, then gets out of the way).</p>","tags":["blog","VPN","Network","Tailscale"]},{"location":"blog/2025/11/08/tailscale---one-tool-less-infrastructure/#why-this-matters","title":"Why This Matters","text":"<p>Networking tools usually force a choice: simple but insecure, or secure but complex. Port forwarding is easy but leaves your services exposed. VPNs are secure but a pain to set up and maintain.</p> <p>Tailscale doesn't make you choose. It's secure by default (WireGuard encryption, identity-based auth, ACL policies) and actually easier to use than other alternatives.</p> <p>You're replacing DNS servers, ngrok subscriptions, firewall configurations, VPN infrastructure, and SSH key management with one tool that takes 2 minutes to set up.</p>","tags":["blog","VPN","Network","Tailscale"]},{"location":"blog/2025/11/08/tailscale---one-tool-less-infrastructure/#get-started","title":"Get Started","text":"<p>Download from tailscale.com/download. Install on two devices. Sign in. They'll find each other automatically.</p> <pre><code># See your devices\ntailscale status\n\n# SSH into another machine\nssh machine-name\n\n# Share something publicly\ntailscale funnel 8080\n\n# Share with just your team\ntailscale serve 8080\n</code></pre> <p>Two minutes to set up. Zero maintenance required.</p>","tags":["blog","VPN","Network","Tailscale"]},{"location":"blog/2025/11/13/choosing-the-right-camera-for-cv-application/","title":"Choosing the Right Camera for CV Application","text":"<p>When building a face recognition\u2013based attendance system, the camera is not just an accessory \u2014 it's the foundation of your entire pipeline. Even the best recognition model can fail if the camera feed is noisy, blurred, or poorly lit. Choosing the right camera requires balancing image quality, cost, and computational performance.</p> <p>Let's walk through what to consider before buying, which specs matter most, and how to find the right balance for your use case. Understand the Application Context Before comparing specs, think carefully about where and how your camera will be installed \u2014 the environment drives nearly every other decision. Face recognition for attendance can happen in two broad contexts: indoor and outdoor. Each has its own design considerations.</p> <p></p>","tags":["blog","Computer Vision","Camera specification","Tech Guide"]},{"location":"blog/2025/11/13/choosing-the-right-camera-for-cv-application/#indoor-installations","title":"Indoor Installations","text":"<p>Indoor locations such as offices, schools, or factory floors generally have controlled lighting and are physically secure.</p> <ul> <li>Here, compact \"cube\" or mini-dome cameras work best \u2014 they blend easily into the environment and save space.</li> <li>Since tampering risk is low, reset-button security is less critical.</li> <li>Focus instead on good field-of-view coverage and stable lighting near the entry area.</li> <li>Still, consider adding soft ambient or IR lighting if operation continues into the night \u2014 consistent illumination keeps face features sharp for recognition.</li> </ul>","tags":["blog","Computer Vision","Camera specification","Tech Guide"]},{"location":"blog/2025/11/13/choosing-the-right-camera-for-cv-application/#outdoor-installations","title":"Outdoor Installations","text":"<p>Outdoor entrances, gates, or parking areas add new challenges \u2014 weather, lighting variation, and security.</p> <ul> <li>Choose weather-resistant (IP66/IP67-rated) housings that can withstand rain, dust, and temperature shifts.</li> <li>Prefer dome or bullet-style PoE IP cameras; domes hide lens direction and are harder to tamper with.</li> <li>The reset button and ports should not be easily accessible from outside \u2014 this prevents unauthorized factory resets.</li> <li>Ensure night-time visibility through either integrated IR LEDs or external lighting.</li> <li>If sunlight causes backlighting issues, enable WDR (Wide Dynamic Range) to maintain face detail.</li> </ul>","tags":["blog","Computer Vision","Camera specification","Tech Guide"]},{"location":"blog/2025/11/13/choosing-the-right-camera-for-cv-application/#key-camera-specs-that-impact-face-recognition","title":"Key Camera Specs That Impact Face Recognition","text":"<p>Choosing a camera for face recognition isn\u2019t about chasing the highest megapixel count \u2014 it\u2019s about finding the balance between clarity, consistency, and computational efficiency. Every specification affects how well your system captures usable facial data and how heavy that stream will be to process on the server.</p> <p>Start with resolution. For most attendance setups, a 2 to 4 megapixel (1080p\u20131440p) camera can provide enough detail to extract accurate facial embeddings at a 2\u20135 meter distance. Higher resolutions like 4K may look impressive but can double your bandwidth and GPU load without improving recognition quality proportionally. </p> <p>Next, check the frame rate. Around 25 to 30 frames per second is ideal \u2014 fast enough to capture people as they move naturally, while avoiding motion blur. Going beyond that rarely improves recognition but increases streaming and storage costs.</p> <p>A good CMOS sensor, performs well in low-light indoor scenes and offers strong dynamic range. Combine this with WDR (Wide Dynamic Range) support \u2014 at least 120 dB if available \u2014 to handle tricky lighting like bright entrances or mixed daylight.</p> <p>The lens defines how your scene is framed. A 2.8 mm\u20134 mm lens gives a wide enough view for entrances, allowing multiple people to be seen clearly at typical working distances. Varifocal lenses are worth the small extra cost if you expect to fine-tune placement later.</p> <p>Finally, consider network and compression. Cameras supporting H.265 or H.265+ codecs reduce bandwidth and storage needs significantly compared with older H.264 streams. If you can, prefer PoE (Power-over-Ethernet) models \u2014 they simplify installation, needing just a single cable for both power and data.</p>","tags":["blog","Computer Vision","Camera specification","Tech Guide"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/","title":"LiveKit - One Platform, Real-Time Everything","text":"<p>Video calls need one platform. Live streaming needs another. Voice AI agents need a third. Computer vision processing? That's a fourth vendor. Managing robots remotely? Good luck finding something that works. LiveKit replaces all of them. One SDK for voice agents, video streaming, vision AI, and robot control. Sub-100ms latency across the board.</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#what-is-livekit","title":"What Is LiveKit?","text":"<p>LiveKit is an open-source real-time communication platform built on WebRTC. It's not just another video calling API - it's infrastructure for anything that needs to move audio, video, or data between users and AI agents in real-time.</p> <p>Voice AI that talks to customers. Video streaming from robots. AI analyzing live camera feeds. Different problems, all in the same session, all using the same platform.</p> <p>No juggling multiple services. No vendor lock-in. No reinventing WebRTC from scratch.</p> <p>Beyond video calls, LiveKit replaces: dedicated streaming CDNs, voice AI infrastructure, robotics communication stacks, and complex WebRTC implementations.</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#how-livekit-works","title":"How LiveKit Works","text":"<p>The Stack:</p> <ul> <li>LiveKit Server - WebRTC SFU (Selective Forwarding Unit) that routes media streams. Self-host or use LiveKit Cloud.</li> <li>Rooms - Where participants connect. Users, AI agents, robots - all join the same room to communicate.</li> <li>Participants - Anything connected to a room. Browser clients, mobile apps, backend agents, IoT devices.</li> <li>Tracks - Audio, video, or data streams published by participants. Subscribe to tracks you want to receive.</li> </ul> <p>Simple flow:</p> <ol> <li>Server creates a room</li> <li>Participants join with access tokens</li> <li>Publish tracks (camera, mic, screen, data)</li> <li>Subscribe to other participants' tracks</li> <li>Server forwards media directly between participants</li> </ol> <pre><code># Backend: Create room and generate token\nroom = await livekit_api.room.create_room(\"my-room\")\ntoken = create_token(identity=\"user1\", room_name=\"my-room\")\n\n# Client: Join and publish\nroom = Room()\nawait room.connect(url, token)\nawait room.local_participant.publish_track(camera_track)\n</code></pre> <p>No central transcoding = Server doesn't decode/re-encode video. Server just forwards packets directly. Low latency, scales horizontally.</p> <p>Agent workers sit between server and AI models. Join rooms as participants, process audio/video, call AI APIs, publish responses back.</p> <p>\ud83d\udcda Architecture overview</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#build-voice-ai-that-actually-works","title":"Build Voice AI That Actually Works","text":"<p>Replaces: Custom STT-LLM-TTS pipelines, latency-prone API chains, fragile state management</p> <p>You know the drill with voice AI: chain together separate APIs for speech-to-text, your LLM, and text-to-speech. Each step adds 300ms of latency. User interrupts mid-sentence? Your state management explodes. Your app crashes overnight because the TTS API went down.</p> <p>LiveKit Agents:</p> <pre><code>from livekit.agents import VoiceAssistant\nfrom livekit.plugins import openai, deepgram, elevenlabs\n\nassistant = VoiceAssistant(\n    stt=deepgram.STT(),\n    llm=openai.LLM(model=\"gpt-4\"),\n    tts=elevenlabs.TTS(),\n)\nassistant.start(room)\n</code></pre> <p>Your voice AI is live. Natural interruptions handled automatically. Turn detection using transformer models. Multi-agent workflows when you need them.</p> <p>ChatGPT's Advanced Voice Mode? Built on LiveKit. Millions of users, every day.</p> <p>Building a phone-based customer service bot? Restaurant ordering system? Medical triage assistant? One framework, production-ready from day one.</p> <p>\ud83d\udcda Voice AI quickstart \u00b7 Agent examples</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#stream-video-without-the-video-streaming-complexity","title":"Stream Video Without the Video Streaming Complexity","text":"<p>Replaces: Traditional CDNs, HLS delays, separate chat infrastructure</p> <p>Setting up HLS streaming: 10-30 seconds of latency, viewers see different things at different times, separate WebSocket server for chat, separate RTMP ingest pipeline, separate viewer analytics.</p> <p>LiveKit's WebRTC Streaming:</p> <pre><code>room = Room()\nawait room.connect(url, token)\n\n# Start streaming\nawait room.local_participant.publish_track(video_track)\n</code></pre> <p>Every viewer is within 250ms of real-time. They all see the same frame at the same moment. Two-way audio/video built-in - any viewer can become a streamer instantly. Chat and data messages included. Record sessions with one API call.</p> <p>\ud83d\udcda Livestreaming docs \u00b7 Recording guide</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#control-robots-from-anywhere","title":"Control Robots From Anywhere","text":"<p>Replaces: Custom video streaming solutions, high-latency feeds, unreliable connections</p> <p>Your robots have cameras. Sensors. Microphones. You need that data streamed to operators in real-time, or processed by AI in the cloud, or both. Building this from scratch means dealing with video encoding, network resilience, secure streaming, and somehow doing it all with under 100ms latency.</p> <p>LiveKit for Robotics:</p> <pre><code># On the robot\ntrack = VideoTrack.from_camera()\nroom.local_participant.publish_track(track)\n\n# Send sensor data\nawait room.local_participant.publish_data(\n    sensor_readings,\n    destination_identities=[\"operator\"]\n)\n</code></pre> <p>Stream from thousands of robots. Route specific feeds to operators. Process video with AI models in real-time. All over unreliable mobile networks - WebRTC handles packet loss, adapts bitrate automatically.</p> <p>Agricultural robots working in fields with spotty connection? WebRTC stays connected where traditional streaming dies.</p> <p>\ud83d\udcda Robotics use case \u00b7 Data streams guide</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#mix-humans-and-ai-in-the-same-call","title":"Mix Humans and AI in the Same Call","text":"<p>Replaces: Separate platforms, awkward transfers, repeating your problem three times</p> <p>Customer calls. AI greets and troubleshoots. Needs billing help. Transfer. Customer explains issue again. Needs technical support. Transfer again. Explain everything from scratch. Again.</p> <p>LiveKit Multi-Agent Workflows:</p> <pre><code>class FrontlineAgent(Agent):\n    @function_tool()\n    async def transfer_to_billing(self):\n        return BillingAgent(chat_ctx=self.chat_ctx)\n\n    @function_tool()\n    async def escalate_to_human(self):\n        return HumanAgent(chat_ctx=self.chat_ctx)\n\n# chat_ctx = full conversation history passes to next agent\n</code></pre> <p>AI greeter \u2192 Billing AI \u2192 Human specialist. Same call. Full context preserved. No repeating. Each agent knows what previous agents discussed.</p> <p>Examples: Medical triage (symptoms \u2192 specialist), drive-thru ordering (greeter \u2192 order taker \u2192 payment), call centers (AI screens \u2192 human closes).</p> <p>\ud83d\udcda Multi-agent workflows \u00b7 Agent handoff examples</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#actually-multimodal-ai","title":"Actually Multimodal AI","text":"<p>Replaces: Voice-only AI, separate video processing pipelines</p> <p>Your AI should see what users see. Point a camera at a product, ask questions about it. Share your screen, get help with what you're looking at. Current solution: send screenshots to vision models, dealing with terrible latency.</p> <p>LiveKit Vision Agents:</p> <pre><code>assistant = MultimodalAgent(\n    video=True,  # Agent can see\n    audio=True,  # Agent can hear\n    llm=openai.LLM(model=\"gpt-4o\"),\n)\n\n# Camera feed goes directly to the agent\n# User speaks, agent sees and responds\n</code></pre> <p>Gemini Live agents that can see. Vision-enabled customer support. AI assistants for virtual events that understand what's on screen. Educational apps where AI tutors watch students solve problems.</p> <p>Video, audio, and data - all in one real-time session with your AI models.</p> <p>\ud83d\udcda Vision agent example \u00b7 Multimodal capabilities</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#deploy-anywhere","title":"Deploy Anywhere","text":"<p>Replaces: Vendor lock-in, inflexible hosting</p> <p>Self-Hosted:</p> <pre><code># Install LiveKit Server (Linux)\ncurl -sSL https://get.livekit.io | bash\n\n# Run it\nlivekit-server --dev\n</code></pre> <p>Full control. Your infrastructure. Your compliance requirements. Apache 2.0 license - modify whatever you need.</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#get-started","title":"Get Started","text":"<p>Try it live: Visit kitt.livekit.io - talk to a real-time voice AI agent. Running on LiveKit. All open source. Just hit the Connect button at top-right.</p> <p>Quick Start (Python):</p> <pre><code>pip install livekit livekit-agents\n\n# Create your first voice agent\npython agent.py dev\n</code></pre> <p>Quick Start (Self-Hosted):</p> <pre><code>docker run -p 7880:7880 \\\n  -e LIVEKIT_KEYS=\"devkey: secret\" \\\n  livekit/livekit-server:latest --dev\n</code></pre> <p>10 minutes to working prototype. Zero maintenance. Unlimited possibilities.</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#real-world-examples","title":"Real-World Examples","text":"<p>Voice AI: Customer service bots that handle thousands of concurrent calls Live Shopping: Interactive auctions with millions in sales</p> <p>Computer Vision: Real-time face detection for attendance systems - process camera feeds, draw bounding boxes, stream annotated video to monitoring dashboards</p> <p>Robotics: Controlling drones or agricultural machines with real-time video and telemetry</p> <p>Education: Virtual classrooms with breakout rooms and screen sharing</p> <p>Events: Interactive livestreams with real-time Q&amp;A</p> <p>One platform. Every real-time use case.</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/12/livekit---one-platform-real-time-everything/#resources","title":"Resources","text":"<p>\ud83d\udcd6 Documentation</p> <p>\ud83d\udcbb GitHub</p> <p>\ud83c\udf93 Free Course (DeepLearning.AI)</p> <p>\ud83d\udd27 Example Agents</p> <p>Start building: livekit.io</p>","tags":["blog","Agent framework","Real-time","Voice AI","WebRTC"]},{"location":"blog/2025/11/23/benchmarking-tabpfn-against-xgboost-and-catboost-on-kaggle-datasets/","title":"Benchmarking TabPFN against XGboost and Catboost on Kaggle datasets","text":"<p>This study benchmarks CatBoost, XGBoost, and TabPFN across multiple Kaggle datasets, including classification, regression, and time series tasks. TabPFN showed strong performance on small datasets, often outperforming the boosting models without any feature engineering or preprocessing. However, it struggles with large datasets over 10,000 rows due to memory limitations, where XGBoost remains more effective, particularly for time series forecasting. Overall, TabPFN is a powerful tool for quick experimentation on smaller tabular datasets but is less practical for large-scale applications.</p>","tags":["blog","Benchmarking","TabPFN","XGBoost","CatBoost","Tabular Data"]},{"location":"blog/2025/11/23/benchmarking-tabpfn-against-xgboost-and-catboost-on-kaggle-datasets/#1-objective","title":"1. Objective","text":"<p>The objective of this study is to benchmark the performance of three models \u2014 CatBoost, XGBoost, and TabPFN \u2014 across multiple Kaggle datasets. The aim is to evaluate how these models perform on diverse tasks including binary classification, regression, and time series forecasting. The focus is on comparing predictive accuracy, error metrics, and general applicability of TabPFN (a transformer-based model designed for tabular data) against well-established gradient boosting models.</p> <ul> <li>Note: For comparison purposes, we did not perform feature engineering, missing   value handling, or extensive data cleaning. All models were trained on the raw   datasets to observe their out-of-the-box capabilities.</li> </ul>","tags":["blog","Benchmarking","TabPFN","XGBoost","CatBoost","Tabular Data"]},{"location":"blog/2025/11/23/benchmarking-tabpfn-against-xgboost-and-catboost-on-kaggle-datasets/#2-datasets","title":"2. Datasets","text":"<p>The experiments were conducted on four Kaggle datasets covering different predictive modeling tasks. A summary of each dataset is provided below:</p> Dataset Name Task Type Target Variable Number of Samples Number of Features Notes Titanic - Machine Learning from Disaster Binary Classification Survived (0/1) 891 (train set) 11 (after cleaning) Predict passenger survival House Prices - Advanced Regression Techniques Regression SalePrice 1,460 79 Predict house sale prices Binary Prediction with a Rainfall Dataset Binary Classification RainTomorrow (Yes/No) 2,190 12 Predict if it will rain tomorrow Forecasting Sticker Sales Time Series Forecasting num_sold 230,130 5 (date, country, store, product, id) Predict sticker sales; missing values present in num_sold","tags":["blog","Benchmarking","TabPFN","XGBoost","CatBoost","Tabular Data"]},{"location":"blog/2025/11/23/benchmarking-tabpfn-against-xgboost-and-catboost-on-kaggle-datasets/#3-results","title":"3. Results","text":"<p>The models were evaluated based on metrics appropriate to each task. For the House Prices dataset, RMSLE (Root Mean Squared Log Error) was used in accordance with Kaggle\u2019s official evaluation method</p> <ul> <li>Note: Due to TabPFN\u2019s limitation of handling a maximum of 10,000 rows, for large   datasets (Sticker Sales), we performed random sampling of 10,000 entries and   further split them into training and validation sets. CatBoost and XGBoost were also   trained on the same sampled data to ensure a fair comparison.</li> </ul> Dataset Name Metric CatBoost XGBoost TabPFN Notes Titanic - Machine Learning from Disaster Accuracy 0.77511 0.73584 0.78947 House Prices - Advanced Regression Techniques RMSLE (Root Mean Squared Log Error) 0.1283 0.15554 0.11359 Evaluated on log(predictions)vslog(actual) as per Kaggle Binary Prediction with a Rainfall Dataset AUC ROC 0.84231 0.8458 0.86564 Forecasting Sticker Sales RMSE 102.08 89.56 99.24 All models were trained on a sampled dataset of 10,000 entries.","tags":["blog","Benchmarking","TabPFN","XGBoost","CatBoost","Tabular Data"]},{"location":"blog/2025/11/23/benchmarking-tabpfn-against-xgboost-and-catboost-on-kaggle-datasets/#4-observations","title":"4. Observations","text":"<ul> <li>Titanic Dataset: TabPFN slightly outperformed CatBoost and XGBoost in terms of   accuracy, showing good performance on smaller classification tasks without feature   engineering.</li> <li>House Prices: TabPFN achieved the lowest RMSLE, indicating strong generalization   ability for regression problems with log-transformed targets, even when trained on raw   data.</li> <li>Rainfall Dataset: TabPFN achieved the highest AUC ROC, outperforming both boosting   models despite no additional data preprocessing or feature engineering.</li> <li>Sticker Sales: XGBoost showed the best performance on time series forecasting.   TabPFN performed reasonably well, but its performance lagged</li> </ul>","tags":["blog","Benchmarking","TabPFN","XGBoost","CatBoost","Tabular Data"]},{"location":"blog/2025/11/23/benchmarking-tabpfn-against-xgboost-and-catboost-on-kaggle-datasets/#5-limitations","title":"5. Limitations","text":"<p>During this benchmarking study, the following limitations were encountered:</p> <ul> <li>TabPFN Row Limitations: TabPFN officially supports datasets with up to 10,000 rows.</li> <li>Handling Large Datasets:</li> <li>For the Sticker Sales dataset (230,130 samples), attempts to train on the full     dataset by ignoring the 10,000-row limit resulted in failures:<ul> <li>On GPU (3.8 GB VRAM), the training failed due to out-of-memory errors.</li> <li>On CPU, running TabPFN on the full dataset caused the kernel to crash.</li> </ul> </li> <li>Therefore, for Sticker Sales, we trained all models on a 10,000-entry random sample to ensure comparability.</li> </ul>","tags":["blog","Benchmarking","TabPFN","XGBoost","CatBoost","Tabular Data"]},{"location":"blog/2025/11/23/benchmarking-tabpfn-against-xgboost-and-catboost-on-kaggle-datasets/#6-conclusion","title":"6. Conclusion","text":"<p>This benchmarking study demonstrates that TabPFN, trained on large-scale synthetic data and leveraging in-context learning, can outperform or match established models like CatBoost and XGBoost on small datasets with minimal preprocessing. Its ability to handle raw tabular data without requiring feature engineering makes it a powerful tool for quick experimentation and baseline modeling.</p> <p>However, significant limitations arise when working with larger datasets beyond 10,000 rows, as noted by the model\u2019s creators. Attempts to run TabPFN on large datasets resulted in memory overloads and kernel crashes, making it impractical for large-scale training.</p> <p>In summary:</p> <ul> <li>TabPFN is highly competitive or even superior on smaller datasets without preprocessing.</li> <li>It is not recommended for large datasets due to its scalability limitations and resource   consumption issues.</li> <li>In domain-specific or structured time series forecasting tasks, models like XGBoost still outperform TabPFN.</li> <li>Future improvements in scaling TabPFN and adapting it for specialized tasks could make   it a strong candidate for broader practical use.</li> </ul>","tags":["blog","Benchmarking","TabPFN","XGBoost","CatBoost","Tabular Data"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/","title":"The State of Uzbek Data for AI","text":"<p>A Comprehensive Overview of Open-Source Uzbek Datasets and Their Roles in LLM Development</p>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#1-introduction","title":"1. Introduction","text":"<p>As artificial intelligence continues to reshape industries worldwide, language remains the foundation of intelligent systems. For Uzbekistan, the future of AI depends on the ability of models to understand, reason, and communicate in Uzbek\u2014a rich language with a rapidly digitalizing presence but still limited representation in machine learning datasets.</p> <p>While English, Chinese, and other models benefit from vast open corpora, Uzbek data remains fragmented and under-structured. This article provides a detailed exploration of available Uzbek datasets, classifies them by type, and explains how each class contributes to building modern large language models (LLMs) and NLP models. It also presents an exploratory data analysis (EDA) of open-source Uzbek data to identify what exists today and what is still missing for true Uzbek AI capability.</p>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#2-why-uzbek-data-matters","title":"2. Why Uzbek Data Matters","text":"<p>Training a large language model is not only about collecting words - it is about teaching language comprehension, task understanding, and reasoning. Each category of dataset serves a distinct function in that process:</p> <ul> <li>Raw text corpora teaches grammar, structure, and contextual flow.  </li> <li>Instruction datasets teach models to follow human directions and respond appropriately.  </li> <li>Chain-of-Thought (CoT) data develop step-by-step reasoning and explainable logic.  </li> <li>Annotated datasets train and evaluate specific skills such as classification or information extraction.  </li> <li>Evaluation datasets are used to measure performance across language understanding, reasoning, and instruction-following tasks. They provide standardized benchmarks for comparing LLMs.</li> </ul> <p>Understanding how much data exists in each category helps organizations and researchers design the next generation of Uzbek-capable AI systems.</p>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#3-classes-of-uzbek-data","title":"3. Classes of Uzbek Data","text":"<p>Based on an examination of open repositories such as Hugging Face, Mendeley Data, and academic corpora, Uzbek data can be grouped into four principal classes:</p> Class Description Representative Datasets Primary Use in Model Training Raw Text Corpora Unstructured collections of text (books, news, and web pages). Tahrirchi/uz-crawl, murodbek/uz-books Pretraining: builds fundamental Uzbek grammar, syntax, and style. Instruction Datasets Prompt-response examples showing how to follow user instructions. UAzimov/uzbek-instruct-llm, Behbudiy/alpaca-cleaned-uz, Behbudiy/translation-instruction Supervised fine-tuning: teaches task execution and conversational behavior. Chain-of-Thought (CoT) Data Multi-step reasoning traces showing how conclusions are reached. (No publicly available Uzbek dataset yet) Reasoning fine-tuning: trains logical, interpretable problem solving. Annotated / Task-Specific Data Labeled datasets for NLP tasks (e.g., sentiment, NER, semantic similarity). Behbudiy/uzbek-sentiment-analysis NER for Uzbek (Mendeley) Evaluation and specialization: supports classification, labeling, or entity recognition. Evaluation datasets Benchmarks used to measure model performance in language understanding, reasoning, and instruction following Tahrirchi/uzlib, SimReIUz Performance assessment \u2014 evaluates comprehension, factual accuracy, fluency, across diverse Uzbek language tasks.","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#4-eda-of-open-source-uzbek-datasets","title":"4. EDA of Open-Source Uzbek Datasets","text":"<p>In linguistic coverage, Uzbek data is sufficient for language modeling: large corpora-like uz-crawl and uz-books ensure grammatical and lexical diversity. However, in reasoning and structured task learning, the resources are minimal. Models trained purely on existing data can speak Uzbek but cannot reason in Uzbek\u2014they lack exposure to instructional patterns, logic sequences, and structured decision-making examples.</p> Dataset Name Estimated Total Volume / Rows Description &amp; Limitation How it was Collected / Source murodbek / uz-books \u2248 49.93 GB text (\u2248 40 000 books) Large literary &amp; academic corpus in both Latin and Cyrillic Uzbek. Excellent for pre-training language fluency and style. Digitized public-domain Uzbek books from online libraries; cleaned and de-duplicated to plain text. (Hugging Face) tahrirchi / uz-crawl \u2248 3.41 GB (raw) \u2192 \u2248 1.7 GB processed / \u2248 30 M sentences Web-crawled Uzbek text (news, blogs, forums). Broad topical diversity but some noise and spelling variation. Automatically crawled and language-filtered; HTML tags removed and duplicates dropped. (Hugging Face) UAzimov / uzbek-instruct-llm \u2248 12.6 MB / \u2248 15 000 prompt\u2013response pairs General instruction dataset (Q\\&amp;A / task-following). Good for initial supervised fine-tuning; lacks domain-specific reasoning. Crowdsourced and translated from English (Alpaca / ShareGPT) prompts; manually cleaned. (Hugging Face) Behbudiy / alpaca-cleaned-uz \u2248 25.1 MB / \u2248 51 760 rows High-quality Uzbek translation of Stanford Alpaca. Enables general instruction-following. Still general-purpose, not legal or analytical. Machine-translated and human-post-edited by Behbudiy Lab from the English Alpaca set. (Hugging Face) Behbudiy / translation-instruction \u2248 13.8 MB / \u2248 20 000 examples Instruction-style parallel data (English \u2194 Uzbek). Useful for bilingual fine-tuning; limited to translation tasks. Derived from existing parallel corpora reframed as \u201cTranslate this sentence into X\u201d prompts. (Hugging Face) Behbudiy / uzbek-sentiment-analysis \u2248 10 MB / \u2248 10 000 labeled samples Supervised sentiment dataset (positive / negative / neutral). Good for evaluation but not for reasoning. Manually labeled reviews and social-media texts from Uzbek online sources. (Hugging Face) UzABSA (Sanatbek / aspect-based-sentiment-analysis-uzbek) \u2248 3 500 reviews / \u2248 6 100 sentences ( ~ 3\u20135 MB ) Aspect-based sentiment dataset for fine-grained opinion mining (restaurant domain). Useful for NLP benchmarking but small scale. Collected from Uzbek restaurant reviews; manually annotated for aspect terms and polarities. (ACL Anthology 2024) SimRelUz (Semantic Relatedness) \u2248 1 MB / 1 418 word pairs Semantic similarity and relatedness ratings for Uzbek word pairs. Too small for training but valuable for embedding evaluation. Word pairs rated (1\u20135 scale) by 11 native Uzbek speakers for similarity and relatedness. (arXiv 2205.06072)","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#5-how-each-class-contributes-to-model-training","title":"5. How Each Class Contributes to Model Training","text":"Model Stage Data Type Objective Pretraining Raw text corpora Establish Uzbek language understanding\u2014grammar, morphology, sentence formation. Supervised Fine-Tuning Instruction datasets Teach the model to interpret user intent and deliver structured responses. Reasoning Fine-Tuning CoT datasets Enable step-by-step legal, mathematical, or procedural reasoning. Evaluation &amp; Domain Adaptation Annotated datasets Benchmark and refine task-specific accuracy (e.g., sentiment, entities, classification). <p>Without instruction and CoT data, a model trained solely on text will appear fluent yet behave superficially\u2014unable to follow directives, justify decisions, or apply rules.</p>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#6-key-findings-from-the-eda","title":"6. Key Findings from the EDA","text":"<ol> <li>The Uzbek data ecosystem is text-rich but behavior-poor.    Most datasets contain free text rather than supervised interactions or reasoning traces.  </li> <li>Instructional data is emerging but not domain-aligned.    Current examples are general rather than specialized (e.g., law, education, healthcare).  </li> <li>Reasoning (CoT) data remains the largest gap.    No open Uzbek dataset provides reasoning or explanation examples\u2014limiting higher-order understanding.  </li> <li>Annotated datasets serve auxiliary purposes.    They are useful for evaluation but cannot teach complex cognitive behavior.</li> </ol>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#7-implications-for-future-uzbek-ai-development","title":"7. Implications for Future Uzbek AI Development","text":"<p>The current landscape demonstrates that Uzbekistan has made progress in linguistic data availability, but still lacks reasoning-oriented, instruction-rich corpora.</p> <p>To advance national AI research and productization:</p> <ul> <li>Develop domain-specific instruction datasets (e.g., legal Q\\&amp;A, civic information, educational tutoring).  </li> <li>Create Chain-of-Thought datasets in Uzbek that show intermediate reasoning steps and explanations.  </li> <li>Establish standardized benchmarks to measure understanding, reasoning, and factual correctness in Uzbek.  </li> <li>Encourage open collaboration between universities, AI labs, and government agencies to share curated, structured data.</li> </ul>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#8-translation-data-for-uzbek-english","title":"8. Translation Data for Uzbek \u2194 English","text":"","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#81-what-translation-data-is","title":"8.1 What Translation Data Is","text":"<p>Translation datasets are called parallel corpora \u2014 collections of sentence pairs (or paragraphs) in two or more languages that express the same meaning.</p> <p>Each record has the structure:</p> <pre><code>{\n  \"source\": \"Men Toshkentda yashayman.\",\n  \"target\": \"I live in Tashkent.\"\n}\n</code></pre> <p>They are used to train and evaluate neural machine translation (NMT) models such as MarianMT, M2M100, or NLLB (No Language Left Behind).</p> <p>In contrast to raw text corpora, which teaches language patterns, parallel data explicitly teach cross-lingual alignment \u2014 how a sentence in Uzbek corresponds semantically and syntactically to one in English (or another language).</p>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#82-types-of-translation-data","title":"8.2 Types of Translation Data","text":"<p>There are three main categories of translation datasets relevant to Uzbek:</p> Type Description Example Datasets Use Cases Parallel Sentence Corpora Sentence-level aligned pairs between Uzbek and other languages (English, Russian, Kazakh). OPUS Tatoeba, Tilde MODEL Corpus, Parallel Uzbek\u2013Kazakh corpus (PMC) Training bilingual NMT models (e.g., English \u2194 Uzbek). Instructional Translation Datasets Parallel text expressed as instruction\u2013response format (translation as a task). Behbudiy/translation-instruction Fine-tuning general LLMs to perform translation via instruction tuning (\u201cTranslate this sentence into English\u201d). Multilingual General Corpora Large web crawls or mixed datasets that include Uzbek among many other languages (often automatically aligned). NLLB (Meta), OPUS GlobalVoices, CCAligned, WikiMatrix Building multilingual or massively multilingual translation systems.","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#_1","title":"The State of Uzbek Data for AI","text":"","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#_2","title":"The State of Uzbek Data for AI","text":"","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#83-overview-of-available-uzbek-translation-resources","title":"8.3 Overview of Available Uzbek Translation Resources","text":"Dataset Source Languages Description / Quality Behbudiy / translation-instruction Hugging Face Uzbek \u2194 English 20k instruction-form translation pairs. Manually curated and cleaned. Useful for LLM instruction fine-tuning. OPUS Tatoeba University of Helsinki Uzbek \u2194 English (and others) A multilingual collection of example sentences and translations. Small, clean, conversational. WikiMatrix Facebook AI Uzbek \u2194 English Sentence-aligned Wikipedia texts. Medium quality; alignment sometimes noisy. CCAligned / CCMatrix Facebook AI Uzbek \u2194 English Large-scale automatically aligned Common Crawl data. Very noisy, requires filtering. NLLB Seed Data Meta (No Language Left Behind project) Uzbek \u2194 200+ languages Used internally by Meta to train NLLB-200 models. Public evaluation set available; raw data not fully open. Parallel Uzbek\u2013Kazakh Corpus Academic dataset (PMC 2024) Uzbek \u2194 Kazakh Sentence-level alignment of administrative and news content; suitable for regional cross-lingual transfer.","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#84-how-these-datasets-are-used","title":"8.4 How These Datasets Are Used","text":"","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#a-for-traditional-machine-translation-nmt","title":"a) For Traditional Machine Translation (NMT)","text":"<p>Parallel Uzbek\u2013English corpora are used to train or fine-tune models like:</p> <ul> <li>MarianMT, mBART, NLLB, or M2M100 </li> <li>Objective: learn direct translation mapping via supervised sequence-to-sequence modeling.</li> </ul>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#b-for-multilingual-llms-and-instruction-fine-tuning","title":"b) For Multilingual LLMs and Instruction Fine-Tuning","text":"<p>Translation pairs can also be reframed as instruction tasks to fine-tune multilingual LLMs.</p> <p>Example format:</p> <pre><code>{\n  \"instruction\": \"Translate the following Uzbek sentence into English.\",\n  \"input\": \"Bu qonun 2023-yil 15-iyulda qabul qilingan.\",\n  \"output\": \"This law was adopted on July 15, 2023.\"\n}\n</code></pre> <p>This method doesn\u2019t just teach the model bilingual equivalence \u2014 it also teaches how to perform translation on request, integrating translation into the model\u2019s conversational behavior.</p>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#c-for-alignment-and-multilingual-embeddings","title":"c) For Alignment and Multilingual Embeddings","text":"<p>Some Uzbek\u2013English corpora are used to train multilingual embedding models (like LaBSE, LASER3). These models learn a shared vector space where semantically similar sentences in both languages have nearby representations. This is essential for multilingual search, cross-lingual RAG systems, or translation memory tools.</p>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#85-quality-and-coverage-challenges","title":"8.5 Quality and Coverage Challenges","text":"<p>Despite progress, Uzbek translation datasets face several limitations:</p> <ol> <li>Data Size \u2014 Most Uzbek\u2194English corpora are small (tens of thousands of pairs), compared to tens of millions for major languages.  </li> <li>Domain Imbalance \u2014 Many pairs come from Wikipedia or news sources; there\u2019s limited legal, technical, or colloquial data.  </li> <li>Alignment Noise \u2014 Automatically aligned datasets (like CCAligned) contain significant mismatches due to inconsistent sentence segmentation.  </li> <li>Script Variation \u2014 Uzbek appears in both Latin and Cyrillic scripts, complicating alignment and requiring preprocessing.  </li> <li>Cultural and Legal Context Loss \u2014 Direct translations often omit context critical for Uzbek-specific meaning (e.g., legal or bureaucratic expressions).</li> </ol>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#_3","title":"The State of Uzbek Data for AI","text":"","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#_4","title":"The State of Uzbek Data for AI","text":"","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#_5","title":"The State of Uzbek Data for AI","text":"","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#_6","title":"The State of Uzbek Data for AI","text":"","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#86-summary-of-translation-data-usefulness","title":"8.6 Summary of Translation Data Usefulness","text":"Model Goal Data Type Needed Example Dataset Why It Helps Basic bilingual translation Parallel corpora OPUS Tatoeba, WikiMatrix Teaches direct mapping between Uzbek and English. LLM instruction fine-tuning Instruction-form translation pairs Behbudiy/translation-instruction Enables conversational translation tasks (\u201cTranslate this text\u2026\u201d). Cross-lingual retrieval or RAG Multilingual embeddings or alignment corpora LaBSE, NLLB evaluation data Allows combining Uzbek and English documents in one semantic space.","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#87-outlook","title":"8.7 Outlook","text":"<p>To advance translation quality and integrate it with high-level reasoning (e.g., bilingual legal assistants), future projects should:</p> <ul> <li>Expand professionally curated bilingual corpora with domain diversity (law, healthcare, government).  </li> <li>Add reasoning-style translation datasets, showing how to translate nuanced or context-heavy Uzbek expressions step-by-step.  </li> <li>Develop script normalization tools to handle Cyrillic\u2194Latin consistency.  </li> <li>Create translation benchmarks specific to Central Asian languages for fair evaluation (BLEU, COMET, ChrF++).</li> </ul> <p>With these improvements, Uzbek can move from being a \u201clow-resource language\u201d to a strategically multilingual language in global AI.</p>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#9-conclusion","title":"9. Conclusion","text":"<p>Uzbek-language AI development has entered a transformative stage. In its early years, progress centered on text collection \u2014 web crawls, literary archives, and digitized books that gave language models their foundational linguistic understanding. Today, the next frontier lies not in more text, but in structured and functional data: resources that teach models not only how Uzbek sounds, but how Uzbek thought and reasoning operate.</p> <p>Through this analysis, it is clear that:</p> <ul> <li>Raw text corpora forms a solid linguistic base for pretraining.  </li> <li>Instruction datasets (like UAzimov/uzbek-instruct-llm and Behbudiy/alpaca-cleaned-uz) provide the first step toward task-following behavior.  </li> <li>Annotated datasets add evaluation capabilities for classification and named-entity recognition tasks.  </li> <li>Chain-of-Thought data\u2014the foundation of reasoning and interpretability\u2014remains a critical missing piece.  </li> <li>Translation datasets (e.g., Behbudiy/translation-instruction, OPUS Tatoeba, WikiMatrix) are becoming an essential bridge between Uzbek and global languages, enabling multilingual LLMs and cross-lingual retrieval systems.</li> </ul> <p>The strategic direction for the Uzbek AI community is now clear:</p> <ul> <li>Develop domain-specific instruction datasets (especially in law, healthcare, and education).  </li> <li>Create reasoning-rich Chain-of-Thought corpora that capture Uzbek analytical processes.  </li> <li>Expand translation and alignment data to ensure interoperability between Uzbek and other major languages.  </li> <li>Build national evaluation benchmarks that measure reasoning quality, factual accuracy, and multilingual performance.  </li> <li>Establish Uzbek GLUE benchmark dataset (UzGLUE) to train LLMs in different tasks.</li> </ul> <p>By shifting focus from raw data collection to structured, high-quality, and explainable datasets, Uzbekistan can evolve from a low-resource language environment into a regional AI innovator\u2014one that promotes ethical, transparent, and locally grounded artificial intelligence aligned with the country\u2019s cultural and linguistic identity.</p>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/2025/11/29/the-state-of-uzbek-data-for-ai/#references","title":"References","text":"<ul> <li>UAzimov / uzbek-instruct-llm \u2014 Hugging Face </li> <li>Behbudiy / alpaca-cleaned-uz \u2014 Hugging Face </li> <li>Behbudiy / translation-instruction \u2014 Hugging Face </li> <li>Behbudiy / uzbek-sentiment-analysis \u2014 Hugging Face </li> <li>Tahrirchi / uz-crawl \u2014 Hugging Face </li> <li>Tahrirchi / uz-books \u2014 Hugging Face </li> <li>Parallel Uzbek\u2013Kazakh Corpus \u2014 PMC </li> <li>SimRelUz \u2014 arXiv </li> <li>NER for Uzbek \u2014 Mendeley Data </li> <li>OPUS Tatoeba \u2014 University of Helsinki </li> <li>WikiMatrix \u2014 Facebook AI</li> </ul>","tags":["blog","Uzbek NLP","LLM Training","Instruction Datasets","Cross-lingual AI","Machine Translation"]},{"location":"blog/archive/2025/11/","title":"2025/11","text":"","tags":["blog"]},{"location":"blog/category/tech-blog/","title":"Tech Blog","text":"","tags":["blog"]}]}